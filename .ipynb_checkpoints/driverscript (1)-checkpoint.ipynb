{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "kwl9S93PxRhC"
   },
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 34
    },
    "colab_type": "code",
    "id": "1ePXXcqWyL9I",
    "outputId": "03a739e5-a6f3-471b-f3a8-3645246ec881"
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'doland dont do press conference'"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "def extract_article(link):\n",
    "  l = []\n",
    "  for s in link.split(\"/\"):\n",
    "    l.append(s)\n",
    "  a = l[-2]\n",
    "  a=a.replace(\"_\",\" \")\n",
    "  return a\n",
    "extract_article(\"https://www.reddit.com/r/india/comments/g7qeec/doland_dont_do_press_conference/\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 204
    },
    "colab_type": "code",
    "id": "BVaGYUQp2qW9",
    "outputId": "b26e0993-f1e1-45bd-abb8-d4918a889f6c"
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "'wget' is not recognized as an internal or external command,\n",
      "operable program or batch file.\n"
     ]
    }
   ],
   "source": [
    "!wget -P /root/input/ -c \"https://s3.amazonaws.com/dl4j-distribution/GoogleNews-vectors-negative300.bin.gz\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 71
    },
    "colab_type": "code",
    "id": "ONPpouDWysnS",
    "outputId": "49efc6f8-fc0a-4182-d7f1-485e1c87d775"
   },
   "outputs": [
    {
     "ename": "SyntaxError",
     "evalue": "(unicode error) 'unicodeescape' codec can't decode bytes in position 2-3: truncated \\UXXXXXXXX escape (<ipython-input-6-3d4124343d98>, line 4)",
     "output_type": "error",
     "traceback": [
      "\u001b[1;36m  File \u001b[1;32m\"<ipython-input-6-3d4124343d98>\"\u001b[1;36m, line \u001b[1;32m4\u001b[0m\n\u001b[1;33m    EMBEDDING_FILE = 'C:\\Users\\pc\\intern/GoogleNews-vectors-negative300.bin.gz' # from above\u001b[0m\n\u001b[1;37m                    ^\u001b[0m\n\u001b[1;31mSyntaxError\u001b[0m\u001b[1;31m:\u001b[0m (unicode error) 'unicodeescape' codec can't decode bytes in position 2-3: truncated \\UXXXXXXXX escape\n"
     ]
    }
   ],
   "source": [
    "from gensim.models import Word2Vec\n",
    "from gensim.models import KeyedVectors\n",
    "import pickle\n",
    "EMBEDDING_FILE = 'C:\\Users\\pc\\intern/GoogleNews-vectors-negative300.bin.gz' # from above\n",
    "word2vec = KeyedVectors.load_word2vec_format(EMBEDDING_FILE, binary=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 238
    },
    "colab_type": "code",
    "id": "c9QIxWUi-weG",
    "outputId": "44a5c5a5-1774-4d5d-e50d-37ab4f4a07b5"
   },
   "outputs": [],
   "source": [
    "vec_rep=pd.read_csv(\"vector_representation3.csv\")\n",
    "w2v_headline = vec_rep.values.tolist()\n",
    "w2v_headline = np.array(w2v_headline)\n",
    "w2v_headline"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 1000
    },
    "colab_type": "code",
    "id": "y4UkzJtj-aK5",
    "outputId": "4a209c66-e0dd-40e3-ef33-e665cada355f"
   },
   "outputs": [],
   "source": [
    "def detect_vec(link):\n",
    "  num_rep = []\n",
    "  article = extract_article(link)\n",
    "  print(article)\n",
    "  vocabulary = list(word2vec.vocab)\n",
    "  w2Vec_word = np.zeros(300,dtype=\"float32\")\n",
    "  for word in article.split(\" \"):\n",
    "    if word  in  vocabulary:\n",
    "      w2Vec_word = np.add(w2Vec_word, word2vec[word])\n",
    "  w2Vec_word = np.divide(w2Vec_word, len(article.split()))\n",
    "  num_rep.append(w2Vec_word)\n",
    "  num_rep = np.array(num_rep)\n",
    "  return num_rep\n",
    "\n",
    "\n",
    "detect_vec(\"https://www.reddit.com/r/india/comments/g7qeec/doland_dont_do_press_conference/\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "Y2_GDM4kLsRZ"
   },
   "outputs": [],
   "source": [
    " #Below libraries are for similarity matrices using sklearn\n",
    "from sklearn.metrics.pairwise import cosine_similarity  \n",
    "from sklearn.metrics import pairwise_distances"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "IAs6Y_Xhb1dL"
   },
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "df = pd.read_csv(\"data1.csv\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "_dzCFVAmB-l_"
   },
   "outputs": [],
   "source": [
    "def avg_w2v_based_model(link, num_similar_items=11):\n",
    "    w2v = detect_vec(link)\n",
    "    print(w2v[0].reshape(1,-1).shape)\n",
    "    couple_dist = pairwise_distances(w2v_headline, w2v[0].reshape(1,-1))\n",
    "    indices = np.argsort(couple_dist.ravel())[0:num_similar_items]\n",
    "    df0 = pd.DataFrame({'label': df['lebel'][indices].values,\n",
    "               'headline':df['news'][indices].values,\n",
    "                'Euclidean similarity with the queried article': couple_dist[indices].ravel()})\n",
    "    print(\"=\"*30,\"Queried article details\",\"=\"*30)\n",
    "    print('headline : ',df['news'][indices[0]])\n",
    "    print(\"\\n\",\"=\"*25,\"Recommended articles : \",\"=\"*23)\n",
    "    #return df.iloc[1:,1]\n",
    "    return df0.iloc[1:,]\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "zYypDYny2ve3"
   },
   "outputs": [],
   "source": [
    "def flair_detector(link):\n",
    "  x= avg_w2v_based_model(link)\n",
    "  d = {};\n",
    "  flair =  \"\"\n",
    "  for i in x[\"label\"]:\n",
    "    if i in d.keys():\n",
    "      \n",
    "      d[i]=d[i]+1\n",
    "      \n",
    "    else:\n",
    "      d[i]=1;\n",
    "  mx=0\n",
    "  for i in x[\"label\"]:\n",
    "    if d[i]>mx:\n",
    "      flair = i\n",
    "      mx =  d[i]\n",
    "   \n",
    "  return flair\n",
    "  \n",
    "  "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 136
    },
    "colab_type": "code",
    "id": "Zy_L32iF4305",
    "outputId": "04ac5c1f-13e9-4d7c-dcb0-d6e9f35efb70"
   },
   "outputs": [],
   "source": [
    "flair_detector(str(\"https://www.reddit.com/r/india/comments/g7nuky/armenian_genocide_memorial_at_the_churchyard_of/\"))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 1000
    },
    "colab_type": "code",
    "id": "4i2Y0aF2P9JJ",
    "outputId": "327e5aa3-322e-4d43-f9a6-16249cebe19e"
   },
   "outputs": [],
   "source": [
    "for i in df['lebel']:\n",
    "  print(i)"
   ]
  }
 ],
 "metadata": {
  "colab": {
   "name": "driverscript.ipynb",
   "provenance": []
  },
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 1
}
