# -*- coding: utf-8 -*-
"""app.py

Automatically generated by Colaboratory.

Original file is located at
    https://colab.research.google.com/drive/1pPvuUn10Vz7WF3DkUKjlM27hS7hn0xKy
"""

import pandas as pd
import numpy as np

def extract_article(link):
  l = []
  for s in link.split("/"):
    l.append(s)
  a = l[-2]
  a=a.replace("_"," ")
  return a
extract_article("https://www.reddit.com/r/india/comments/g7qeec/doland_dont_do_press_conference/")

!wget -P /root/input/ -c "https://s3.amazonaws.com/dl4j-distribution/GoogleNews-vectors-negative300.bin.gz"

from gensim.models import Word2Vec
from gensim.models import KeyedVectors
import pickle
EMBEDDING_FILE = '/root/input/GoogleNews-vectors-negative300.bin.gz' # from above
word2vec = KeyedVectors.load_word2vec_format(EMBEDDING_FILE, binary=True)

vec_rep=pd.read_csv("vector_representation3.csv")
w2v_headline = vec_rep.values.tolist()
w2v_headline = np.array(w2v_headline)
w2v_headline

def detect_vec(link):
  num_rep = []
  article = extract_article(link)
  print(article)
  vocabulary = list(word2vec.vocab)
  w2Vec_word = np.zeros(300,dtype="float32")
  for word in article.split(" "):
    if word  in  vocabulary:
      w2Vec_word = np.add(w2Vec_word, word2vec[word])
  w2Vec_word = np.divide(w2Vec_word, len(article.split()))
  num_rep.append(w2Vec_word)
  num_rep = np.array(num_rep)
  return num_rep


detect_vec("https://www.reddit.com/r/india/comments/g7qeec/doland_dont_do_press_conference/")

#Below libraries are for similarity matrices using sklearn
from sklearn.metrics.pairwise import cosine_similarity  
from sklearn.metrics import pairwise_distances

import pandas as pd
df = pd.read_csv("data1.csv")

def avg_w2v_based_model(link, num_similar_items=11):
    w2v = detect_vec(link)
    print(w2v[0].reshape(1,-1).shape)
    couple_dist = pairwise_distances(w2v_headline, w2v[0].reshape(1,-1))
    indices = np.argsort(couple_dist.ravel())[0:num_similar_items]
    df0 = pd.DataFrame({'label': df['lebel'][indices].values,
               'headline':df['news'][indices].values,
                'Euclidean similarity with the queried article': couple_dist[indices].ravel()})
    print("="*30,"Queried article details","="*30)
    print('headline : ',df['news'][indices[0]])
    print("\n","="*25,"Recommended articles : ","="*23)
    #return df.iloc[1:,1]
    return df0.iloc[1:,]

def flair_detector(link):
  x= avg_w2v_based_model(link)
  d = {};
  flair =  ""
  for i in x["label"]:
    if i in d.keys():
      
      d[i]=d[i]+1
      
    else:
      d[i]=1;
  mx=0
  for i in x["label"]:
    if d[i]>mx:
      flair = i
      mx =  d[i]
   
  return flair

flair_detector(str("https://www.reddit.com/r/india/comments/g7nuky/armenian_genocide_memorial_at_the_churchyard_of/"))

!pip install flask-ngrok
from flask_ngrok import run_with_ngrok

from flask  import Flask,render_template,redirect,request


app = Flask(__name__)
run_with_ngrok(app) 

@app.route('/')
def hello():
    return render_template("index.html",my_friends =  friends)

@app.route('/about')
def about():
    return "<h1>about page</h2>"
@app.route('/home')
def home():
    return redirect('/')

titles = []
@app.route('/submit',methods = ["POST"])
def submit_data():
    title = "sa"
    if request.method == "POST":
        link = request.form["link"]
        #page = requests.get(link)
        #soup = BeautifulSoup(page.content,'html.parser')
       # find_all= soup.find_all('h1' ,{"class":"_eYtD2XCVieq6emjKBH3m"})   #extracting title of post 
        flair = flair_detector(link)
        
    return render_template("index.html",flair_is=flair)
       
    

if __name__=='__main__':
    
    app.run()